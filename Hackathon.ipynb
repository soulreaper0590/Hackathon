{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hackathon.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUUg4ttVO-D0",
        "colab_type": "text"
      },
      "source": [
        "# Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECXV_w6HK48l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43_IPBfMN0P0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRUyduecOLKE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "from wordcloud import WordCloud, STOPWORDS"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJsa5KP5OqLG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.read_csv(\"Tweets.csv\")"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CA7kuJVbJTR",
        "colab_type": "text"
      },
      "source": [
        "# Describing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erJBcpSSO3mg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cd2538b0-b970-4a60-bef0-c9dac1c614de"
      },
      "source": [
        "data.shape"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(14640, 15)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5M2njj_HPJSn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "outputId": "ad7bff04-8bae-4f17-b1eb-57f43335858b"
      },
      "source": [
        "data.describe()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>airline_sentiment_confidence</th>\n",
              "      <th>negativereason_confidence</th>\n",
              "      <th>retweet_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1.464000e+04</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>10522.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>5.692184e+17</td>\n",
              "      <td>0.900169</td>\n",
              "      <td>0.638298</td>\n",
              "      <td>0.082650</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>7.791112e+14</td>\n",
              "      <td>0.162830</td>\n",
              "      <td>0.330440</td>\n",
              "      <td>0.745778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>5.675883e+17</td>\n",
              "      <td>0.335000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>5.685592e+17</td>\n",
              "      <td>0.692300</td>\n",
              "      <td>0.360600</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>5.694779e+17</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.670600</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>5.698905e+17</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>5.703106e+17</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>44.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           tweet_id  ...  retweet_count\n",
              "count  1.464000e+04  ...   14640.000000\n",
              "mean   5.692184e+17  ...       0.082650\n",
              "std    7.791112e+14  ...       0.745778\n",
              "min    5.675883e+17  ...       0.000000\n",
              "25%    5.685592e+17  ...       0.000000\n",
              "50%    5.694779e+17  ...       0.000000\n",
              "75%    5.698905e+17  ...       0.000000\n",
              "max    5.703106e+17  ...      44.000000\n",
              "\n",
              "[8 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbIDMB6Ibv-K",
        "colab_type": "text"
      },
      "source": [
        "# Removing Everything except Text and Sentiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dfol3AsAPO2l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = data.loc[:,[\"text\",\"airline_sentiment\"]]"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EiGut1ZPPs_v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for k,tx in enumerate(data[\"text\"]):\n",
        "    li = \" \".join([word for word in tx.split() if 'http' not in word and not word.startswith('@')and word != 'RT'])\n",
        "    data.loc[k,\"text\"] = li"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLMJkNIBTWfc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y = data[\"airline_sentiment\"].apply(lambda x: 0 if x == \"negative\"  else (1 if (x == \"nuetral\") else 2))"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlvUNHdgRGJ-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "c2461e57-70ec-47eb-82e6-be8121abc23c"
      },
      "source": [
        "data[\"airline_sentiment\"].value_counts()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "negative    9178\n",
              "neutral     3099\n",
              "positive    2363\n",
              "Name: airline_sentiment, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ap0Jvb5YSq4I",
        "colab_type": "text"
      },
      "source": [
        "# Text mining\n",
        "1. Removing  Puntuation\n",
        "2. Removing Symbols and Digits\n",
        "2. Removing Stopword, \n",
        "3. Stemming/lemmatizing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mas1cgcQVJBI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "punc = '''!()-[]{};:'\"\\, <>./?@#$%^&*_~'''\n",
        "\n",
        "for k,tx in enumerate(data[\"text\"]):\n",
        "    res = re.sub(r'[^\\w\\s]', '', tx)\n",
        "    data.loc[k,\"text\"] = ''.join([i.lower() for i in res if not i.isdigit()])"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdtmoXWaYpDP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "96d51925-bb39-4a73-d4f8-0965ae845575"
      },
      "source": [
        "nltk.download(\"stopwords\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3Df2E7vgSDh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "stops = stopwords.words('english')"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGd0iAFGgYio",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for k,tx in enumerate(data[\"text\"]):\n",
        "    li = \" \".join([word for word in tx.split() if word not in stops])\n",
        "    data.loc[k,\"text\"] = li"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGkaftB3lwZq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "d9fa8d04-10c8-4a68-a4ba-c0808324f18a"
      },
      "source": [
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "ltz = WordNetLemmatizer()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPIy1HB7nROZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for k,tx in enumerate(data[\"text\"]):\n",
        "    li = \" \".join([ltz.lemmatize(word) for word in tx.split()])\n",
        "    data.loc[k,\"text\"] = li"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTite4pKhdre",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = data[\"text\"]"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tp_93B0vdnL0",
        "colab_type": "text"
      },
      "source": [
        "# "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mc_MGzERg58f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cv = CountVectorizer(lowercase=True, stop_words='english', min_df=2)\n",
        "cvs = cv.fit_transform(X.values.astype('U'))\n",
        "X_cv_df = pd.DataFrame(cvs.toarray(), columns=cv.get_feature_names())"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWLi34PZh-bI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c30ed34a-5178-4389-8a8d-4126f3d1fa72"
      },
      "source": [
        "X_cv_df.shape"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(14640, 4974)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1Tnl4O7iQkt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X_cv_df,Y,test_size = 0.1, random_state= 10)"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Djc4NBvXiAzp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c85b7e2d-29b7-4434-ebae-36f1bea5d6aa"
      },
      "source": [
        "model = RandomForestClassifier(n_estimators=100)\n",
        "model.fit(X_train,Y_train)\n",
        "print(\"Train Accuracy : \" + str(model.score(X_train,Y_train)), \"Test Accuracy : \" + str(model.score(X_test,Y_test)))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Accuracy : 0.9958257437765634 Test Accuracy : 0.7971311475409836\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tBKvFImjqXb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "df2f2339-94e3-44f9-9468-8f79d64b8d6f"
      },
      "source": [
        "model = XGBClassifier(n_estimators = 150)\n",
        "model.fit(X_train,Y_train)\n",
        "print(\"Train Accuracy : \" + str(model.score(X_train,Y_train)), \"Test Accuracy : \" + str(model.score(X_test,Y_test)))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Accuracy : 0.73816029143898 Test Accuracy : 0.7199453551912568\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkv9GbxvlJQW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "493e03af-bf92-46d7-ecdb-92b7b191dedc"
      },
      "source": [
        "model = DecisionTreeClassifier()\n",
        "model.fit(X_train,Y_train)\n",
        "print(\"Train Accuracy : \" + str(model.score(X_train,Y_train)), \"Test Accuracy : \" + str(model.score(X_test,Y_test)))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Accuracy : 0.9959016393442623 Test Accuracy : 0.7342896174863388\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bExjbrwln24a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BusHC8oCn4wa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def onehot(arr):\n",
        "    li = np.zeros((3,len(arr)))\n",
        "    for i,k in enumerate(arr):\n",
        "        hot = [0,0,0]\n",
        "        # print(k)\n",
        "        hot[k] = 1\n",
        "        li[:,i] = hot\n",
        "    return li\n",
        "Y_train = onehot(Y_train)\n",
        "Y_test = onehot(Y_test)"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6moCu6NtSG8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y_test = Y_test.transpose()\n",
        "Y_train = Y_train.transpose()"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgWQdl8LHliA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a074b61a-c861-4f94-98e6-f737facd892c"
      },
      "source": [
        "Y_train.shape"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(13176, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5bAjPf9vmms",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3a31dc13-3bc5-4339-dcfe-c7ae794c32e9"
      },
      "source": [
        "X_train = np.array(X_train)\n",
        "X_train.shape"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(13176, 4974)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBLGvecMyDWI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from keras.layers import Dropout\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fc9rj9L7lQ9G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(1000, input_dim=X_train.shape[1], activation='relu'))\n",
        "model.add(Dropout(0.9))\n",
        "# model.add(Dense(300, activation='relu'))\n",
        "model.add(Dense(100, activation='relu'))\n",
        "model.add(Dropout(0.7))\n",
        "model.add(Dense(10,activation=\"relu\"))\n",
        "model.add(Dense(3,activation= \"sigmoid\"))\n",
        "checkpoint = ModelCheckpoint('model_cv_nn.h5', verbose=1, monitor='val_accuracy',save_best_only=True, mode='auto')"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyoaIQLFuDE4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVw2k_57ue1W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 689
        },
        "outputId": "bea481a5-dc5f-4000-8652-504b2e158651"
      },
      "source": [
        "history = model.fit(X_train, Y_train,validation_data = (X_test,Y_test),callbacks = [checkpoint],epochs=10, batch_size=64)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "203/206 [============================>.] - ETA: 0s - loss: 0.6299 - accuracy: 0.6626\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.79577, saving model to model_cv_nn.h5\n",
            "206/206 [==============================] - 1s 6ms/step - loss: 0.6289 - accuracy: 0.6642 - val_loss: 0.4709 - val_accuracy: 0.7958\n",
            "Epoch 2/10\n",
            "205/206 [============================>.] - ETA: 0s - loss: 0.4446 - accuracy: 0.8044\n",
            "Epoch 00002: val_accuracy improved from 0.79577 to 0.81762, saving model to model_cv_nn.h5\n",
            "206/206 [==============================] - 1s 5ms/step - loss: 0.4451 - accuracy: 0.8045 - val_loss: 0.4091 - val_accuracy: 0.8176\n",
            "Epoch 3/10\n",
            "206/206 [==============================] - ETA: 0s - loss: 0.3873 - accuracy: 0.8367\n",
            "Epoch 00003: val_accuracy improved from 0.81762 to 0.82377, saving model to model_cv_nn.h5\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.3873 - accuracy: 0.8367 - val_loss: 0.3947 - val_accuracy: 0.8238\n",
            "Epoch 4/10\n",
            "194/206 [===========================>..] - ETA: 0s - loss: 0.3556 - accuracy: 0.8535\n",
            "Epoch 00004: val_accuracy improved from 0.82377 to 0.82650, saving model to model_cv_nn.h5\n",
            "206/206 [==============================] - 1s 5ms/step - loss: 0.3550 - accuracy: 0.8530 - val_loss: 0.3882 - val_accuracy: 0.8265\n",
            "Epoch 5/10\n",
            "204/206 [============================>.] - ETA: 0s - loss: 0.3250 - accuracy: 0.8654\n",
            "Epoch 00005: val_accuracy improved from 0.82650 to 0.82787, saving model to model_cv_nn.h5\n",
            "206/206 [==============================] - 2s 8ms/step - loss: 0.3255 - accuracy: 0.8652 - val_loss: 0.3939 - val_accuracy: 0.8279\n",
            "Epoch 6/10\n",
            "201/206 [============================>.] - ETA: 0s - loss: 0.3022 - accuracy: 0.8797\n",
            "Epoch 00006: val_accuracy improved from 0.82787 to 0.82923, saving model to model_cv_nn.h5\n",
            "206/206 [==============================] - 1s 5ms/step - loss: 0.3023 - accuracy: 0.8796 - val_loss: 0.4008 - val_accuracy: 0.8292\n",
            "Epoch 7/10\n",
            "205/206 [============================>.] - ETA: 0s - loss: 0.2831 - accuracy: 0.8848\n",
            "Epoch 00007: val_accuracy did not improve from 0.82923\n",
            "206/206 [==============================] - 1s 4ms/step - loss: 0.2838 - accuracy: 0.8844 - val_loss: 0.4028 - val_accuracy: 0.8292\n",
            "Epoch 8/10\n",
            "205/206 [============================>.] - ETA: 0s - loss: 0.2722 - accuracy: 0.8940\n",
            "Epoch 00008: val_accuracy did not improve from 0.82923\n",
            "206/206 [==============================] - 1s 4ms/step - loss: 0.2721 - accuracy: 0.8939 - val_loss: 0.4133 - val_accuracy: 0.8224\n",
            "Epoch 9/10\n",
            "200/206 [============================>.] - ETA: 0s - loss: 0.2557 - accuracy: 0.8998\n",
            "Epoch 00009: val_accuracy did not improve from 0.82923\n",
            "206/206 [==============================] - 1s 4ms/step - loss: 0.2551 - accuracy: 0.8998 - val_loss: 0.4310 - val_accuracy: 0.8217\n",
            "Epoch 10/10\n",
            "192/206 [==========================>...] - ETA: 0s - loss: 0.2420 - accuracy: 0.9058\n",
            "Epoch 00010: val_accuracy did not improve from 0.82923\n",
            "206/206 [==============================] - 1s 4ms/step - loss: 0.2425 - accuracy: 0.9063 - val_loss: 0.4303 - val_accuracy: 0.8251\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2JSHLXr4vDD",
        "colab_type": "text"
      },
      "source": [
        "# Highest Test accuarcy 82.8% percent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_aycbMPovbpG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tfv = TfidfVectorizer(lowercase=True, stop_words='english', min_df=2)\n",
        "tfvs = tfv.fit_transform(X.values.astype('U'))\n",
        "X_tf_df = pd.DataFrame(tfvs.toarray(), columns=tfv.get_feature_names())"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kv_VX1N05IhZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "outputId": "1e8ce90f-e9e5-49f6-8ea5-fbefb8583caf"
      },
      "source": [
        "X_tf_df.describe()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>aa</th>\n",
              "      <th>aaba</th>\n",
              "      <th>aadfw</th>\n",
              "      <th>aadv</th>\n",
              "      <th>aadvantage</th>\n",
              "      <th>aafail</th>\n",
              "      <th>aal</th>\n",
              "      <th>aano</th>\n",
              "      <th>abandoned</th>\n",
              "      <th>abc</th>\n",
              "      <th>ability</th>\n",
              "      <th>able</th>\n",
              "      <th>aboard</th>\n",
              "      <th>abq</th>\n",
              "      <th>abroad</th>\n",
              "      <th>absolute</th>\n",
              "      <th>absolutely</th>\n",
              "      <th>absurd</th>\n",
              "      <th>abt</th>\n",
              "      <th>abused</th>\n",
              "      <th>abysmal</th>\n",
              "      <th>ac</th>\n",
              "      <th>accept</th>\n",
              "      <th>acceptable</th>\n",
              "      <th>accepted</th>\n",
              "      <th>accepting</th>\n",
              "      <th>access</th>\n",
              "      <th>accident</th>\n",
              "      <th>accidentally</th>\n",
              "      <th>accommodate</th>\n",
              "      <th>accommodated</th>\n",
              "      <th>accommodating</th>\n",
              "      <th>accommodation</th>\n",
              "      <th>accomplished</th>\n",
              "      <th>according</th>\n",
              "      <th>accordingly</th>\n",
              "      <th>account</th>\n",
              "      <th>accountability</th>\n",
              "      <th>acct</th>\n",
              "      <th>accts</th>\n",
              "      <th>...</th>\n",
              "      <th>yard</th>\n",
              "      <th>yay</th>\n",
              "      <th>yea</th>\n",
              "      <th>yeah</th>\n",
              "      <th>year</th>\n",
              "      <th>yearly</th>\n",
              "      <th>yell</th>\n",
              "      <th>yelled</th>\n",
              "      <th>yelling</th>\n",
              "      <th>yep</th>\n",
              "      <th>yes</th>\n",
              "      <th>yesso</th>\n",
              "      <th>yest</th>\n",
              "      <th>yesterday</th>\n",
              "      <th>yikes</th>\n",
              "      <th>yo</th>\n",
              "      <th>york</th>\n",
              "      <th>youd</th>\n",
              "      <th>youi</th>\n",
              "      <th>youll</th>\n",
              "      <th>young</th>\n",
              "      <th>younger</th>\n",
              "      <th>youre</th>\n",
              "      <th>yousuck</th>\n",
              "      <th>youth</th>\n",
              "      <th>youve</th>\n",
              "      <th>youyou</th>\n",
              "      <th>yow</th>\n",
              "      <th>yr</th>\n",
              "      <th>yuck</th>\n",
              "      <th>yuma</th>\n",
              "      <th>yummy</th>\n",
              "      <th>yup</th>\n",
              "      <th>yvonne</th>\n",
              "      <th>yvr</th>\n",
              "      <th>yyz</th>\n",
              "      <th>zero</th>\n",
              "      <th>zone</th>\n",
              "      <th>zoom</th>\n",
              "      <th>zurich</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.005290</td>\n",
              "      <td>0.000071</td>\n",
              "      <td>0.000051</td>\n",
              "      <td>0.000049</td>\n",
              "      <td>0.000279</td>\n",
              "      <td>0.000066</td>\n",
              "      <td>0.000074</td>\n",
              "      <td>0.000049</td>\n",
              "      <td>0.000061</td>\n",
              "      <td>0.000232</td>\n",
              "      <td>0.000216</td>\n",
              "      <td>0.002821</td>\n",
              "      <td>0.000124</td>\n",
              "      <td>0.000271</td>\n",
              "      <td>0.000073</td>\n",
              "      <td>0.000669</td>\n",
              "      <td>0.000952</td>\n",
              "      <td>0.000427</td>\n",
              "      <td>0.000192</td>\n",
              "      <td>0.000069</td>\n",
              "      <td>0.000177</td>\n",
              "      <td>0.000242</td>\n",
              "      <td>0.000397</td>\n",
              "      <td>0.000664</td>\n",
              "      <td>0.000235</td>\n",
              "      <td>0.000304</td>\n",
              "      <td>0.000941</td>\n",
              "      <td>0.000197</td>\n",
              "      <td>0.000106</td>\n",
              "      <td>0.000454</td>\n",
              "      <td>0.000062</td>\n",
              "      <td>0.000234</td>\n",
              "      <td>0.000277</td>\n",
              "      <td>0.000110</td>\n",
              "      <td>0.000321</td>\n",
              "      <td>0.000137</td>\n",
              "      <td>0.001796</td>\n",
              "      <td>0.000193</td>\n",
              "      <td>0.000285</td>\n",
              "      <td>0.000051</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000058</td>\n",
              "      <td>0.000393</td>\n",
              "      <td>0.000398</td>\n",
              "      <td>0.001189</td>\n",
              "      <td>0.003128</td>\n",
              "      <td>0.000066</td>\n",
              "      <td>0.000085</td>\n",
              "      <td>0.000237</td>\n",
              "      <td>0.000120</td>\n",
              "      <td>0.000515</td>\n",
              "      <td>0.006069</td>\n",
              "      <td>0.000072</td>\n",
              "      <td>0.000122</td>\n",
              "      <td>0.002524</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>0.000497</td>\n",
              "      <td>0.000357</td>\n",
              "      <td>0.000485</td>\n",
              "      <td>0.000082</td>\n",
              "      <td>0.000694</td>\n",
              "      <td>0.000245</td>\n",
              "      <td>0.000060</td>\n",
              "      <td>0.003921</td>\n",
              "      <td>0.000134</td>\n",
              "      <td>0.000088</td>\n",
              "      <td>0.001441</td>\n",
              "      <td>0.000125</td>\n",
              "      <td>0.000066</td>\n",
              "      <td>0.001015</td>\n",
              "      <td>0.000067</td>\n",
              "      <td>0.000161</td>\n",
              "      <td>0.000071</td>\n",
              "      <td>0.000304</td>\n",
              "      <td>0.000054</td>\n",
              "      <td>0.000196</td>\n",
              "      <td>0.000352</td>\n",
              "      <td>0.000834</td>\n",
              "      <td>0.000262</td>\n",
              "      <td>0.000054</td>\n",
              "      <td>0.000104</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.042057</td>\n",
              "      <td>0.006084</td>\n",
              "      <td>0.004352</td>\n",
              "      <td>0.004223</td>\n",
              "      <td>0.010715</td>\n",
              "      <td>0.005687</td>\n",
              "      <td>0.006477</td>\n",
              "      <td>0.004223</td>\n",
              "      <td>0.005351</td>\n",
              "      <td>0.010827</td>\n",
              "      <td>0.010144</td>\n",
              "      <td>0.032248</td>\n",
              "      <td>0.007822</td>\n",
              "      <td>0.011102</td>\n",
              "      <td>0.006214</td>\n",
              "      <td>0.017728</td>\n",
              "      <td>0.022798</td>\n",
              "      <td>0.014624</td>\n",
              "      <td>0.008896</td>\n",
              "      <td>0.005986</td>\n",
              "      <td>0.009917</td>\n",
              "      <td>0.009842</td>\n",
              "      <td>0.012651</td>\n",
              "      <td>0.017067</td>\n",
              "      <td>0.010612</td>\n",
              "      <td>0.012545</td>\n",
              "      <td>0.018756</td>\n",
              "      <td>0.009941</td>\n",
              "      <td>0.006453</td>\n",
              "      <td>0.013371</td>\n",
              "      <td>0.005483</td>\n",
              "      <td>0.012012</td>\n",
              "      <td>0.011284</td>\n",
              "      <td>0.007800</td>\n",
              "      <td>0.011947</td>\n",
              "      <td>0.008598</td>\n",
              "      <td>0.025276</td>\n",
              "      <td>0.009599</td>\n",
              "      <td>0.010974</td>\n",
              "      <td>0.004346</td>\n",
              "      <td>...</td>\n",
              "      <td>0.004987</td>\n",
              "      <td>0.014824</td>\n",
              "      <td>0.014229</td>\n",
              "      <td>0.022452</td>\n",
              "      <td>0.033299</td>\n",
              "      <td>0.005693</td>\n",
              "      <td>0.005931</td>\n",
              "      <td>0.010254</td>\n",
              "      <td>0.007504</td>\n",
              "      <td>0.016134</td>\n",
              "      <td>0.052720</td>\n",
              "      <td>0.006194</td>\n",
              "      <td>0.007431</td>\n",
              "      <td>0.030511</td>\n",
              "      <td>0.009133</td>\n",
              "      <td>0.015062</td>\n",
              "      <td>0.012058</td>\n",
              "      <td>0.013637</td>\n",
              "      <td>0.007034</td>\n",
              "      <td>0.018078</td>\n",
              "      <td>0.009965</td>\n",
              "      <td>0.005111</td>\n",
              "      <td>0.038188</td>\n",
              "      <td>0.008112</td>\n",
              "      <td>0.006216</td>\n",
              "      <td>0.023532</td>\n",
              "      <td>0.010680</td>\n",
              "      <td>0.005638</td>\n",
              "      <td>0.018886</td>\n",
              "      <td>0.005717</td>\n",
              "      <td>0.009902</td>\n",
              "      <td>0.006098</td>\n",
              "      <td>0.012791</td>\n",
              "      <td>0.004618</td>\n",
              "      <td>0.008996</td>\n",
              "      <td>0.012512</td>\n",
              "      <td>0.017238</td>\n",
              "      <td>0.010058</td>\n",
              "      <td>0.004621</td>\n",
              "      <td>0.006399</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>0.811421</td>\n",
              "      <td>0.530481</td>\n",
              "      <td>0.372337</td>\n",
              "      <td>0.361347</td>\n",
              "      <td>0.493033</td>\n",
              "      <td>0.516293</td>\n",
              "      <td>0.658825</td>\n",
              "      <td>0.361347</td>\n",
              "      <td>0.557820</td>\n",
              "      <td>0.613946</td>\n",
              "      <td>0.645185</td>\n",
              "      <td>0.735145</td>\n",
              "      <td>0.683375</td>\n",
              "      <td>0.561279</td>\n",
              "      <td>0.560837</td>\n",
              "      <td>0.712762</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.696412</td>\n",
              "      <td>0.524695</td>\n",
              "      <td>0.588968</td>\n",
              "      <td>0.744023</td>\n",
              "      <td>0.489461</td>\n",
              "      <td>0.543682</td>\n",
              "      <td>0.693523</td>\n",
              "      <td>0.795147</td>\n",
              "      <td>0.683032</td>\n",
              "      <td>0.572444</td>\n",
              "      <td>0.700831</td>\n",
              "      <td>0.428936</td>\n",
              "      <td>0.482266</td>\n",
              "      <td>0.565355</td>\n",
              "      <td>0.840904</td>\n",
              "      <td>0.559511</td>\n",
              "      <td>0.649497</td>\n",
              "      <td>0.580662</td>\n",
              "      <td>0.740448</td>\n",
              "      <td>0.560789</td>\n",
              "      <td>0.542699</td>\n",
              "      <td>0.482168</td>\n",
              "      <td>0.385708</td>\n",
              "      <td>...</td>\n",
              "      <td>0.455103</td>\n",
              "      <td>0.748017</td>\n",
              "      <td>0.738265</td>\n",
              "      <td>0.824041</td>\n",
              "      <td>0.830116</td>\n",
              "      <td>0.503881</td>\n",
              "      <td>0.438937</td>\n",
              "      <td>0.526999</td>\n",
              "      <td>0.622738</td>\n",
              "      <td>0.846417</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.529946</td>\n",
              "      <td>0.511915</td>\n",
              "      <td>0.795425</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.838607</td>\n",
              "      <td>0.519154</td>\n",
              "      <td>0.546563</td>\n",
              "      <td>0.611474</td>\n",
              "      <td>0.703829</td>\n",
              "      <td>0.463303</td>\n",
              "      <td>0.456391</td>\n",
              "      <td>0.722792</td>\n",
              "      <td>0.556448</td>\n",
              "      <td>0.512377</td>\n",
              "      <td>0.592361</td>\n",
              "      <td>0.913780</td>\n",
              "      <td>0.524889</td>\n",
              "      <td>0.536196</td>\n",
              "      <td>0.510378</td>\n",
              "      <td>0.760025</td>\n",
              "      <td>0.534505</td>\n",
              "      <td>0.762366</td>\n",
              "      <td>0.408540</td>\n",
              "      <td>0.450055</td>\n",
              "      <td>0.600728</td>\n",
              "      <td>0.478956</td>\n",
              "      <td>0.454655</td>\n",
              "      <td>0.399599</td>\n",
              "      <td>0.462286</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 4974 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                 aa          aaba  ...          zoom        zurich\n",
              "count  14640.000000  14640.000000  ...  14640.000000  14640.000000\n",
              "mean       0.005290      0.000071  ...      0.000054      0.000104\n",
              "std        0.042057      0.006084  ...      0.004621      0.006399\n",
              "min        0.000000      0.000000  ...      0.000000      0.000000\n",
              "25%        0.000000      0.000000  ...      0.000000      0.000000\n",
              "50%        0.000000      0.000000  ...      0.000000      0.000000\n",
              "75%        0.000000      0.000000  ...      0.000000      0.000000\n",
              "max        0.811421      0.530481  ...      0.399599      0.462286\n",
              "\n",
              "[8 rows x 4974 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73F0_70x5UIi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(X_tf_df,Y,test_size = 0.1, random_state= 10)"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXj3tyH85h9U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4b4c13d4-995d-4a1c-9012-fec7827a7690"
      },
      "source": [
        "model = RandomForestClassifier(n_estimators=100)\n",
        "model.fit(X_train,Y_train)\n",
        "print(\"Train Accuracy : \" + str(model.score(X_train,Y_train)), \"Test Accuracy : \" + str(model.score(X_test,Y_test)))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Accuracy : 0.9959016393442623 Test Accuracy : 0.8073770491803278\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzPTPGzM56X5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b2af828c-f0f7-494e-c568-ec5ce0d038f9"
      },
      "source": [
        "model = XGBClassifier(n_estimators = 150)\n",
        "model.fit(X_train,Y_train)\n",
        "print(\"Train Accuracy : \" + str(model.score(X_train,Y_train)), \"Test Accuracy : \" + str(model.score(X_test,Y_test)))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Accuracy : 0.7491651487553127 Test Accuracy : 0.7288251366120219\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCMqak1l5-y-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8ed53dc6-ff76-41be-e1f9-47ace0b03eb5"
      },
      "source": [
        "model = DecisionTreeClassifier()\n",
        "model.fit(X_train,Y_train)\n",
        "print(\"Train Accuracy : \" + str(model.score(X_train,Y_train)), \"Test Accuracy : \" + str(model.score(X_test,Y_test)))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Accuracy : 0.9959016393442623 Test Accuracy : 0.7527322404371585\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTrgiUgb8bi6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def onehot(arr):\n",
        "    li = np.zeros((3,len(arr)))\n",
        "    for i,k in enumerate(arr):\n",
        "        hot = [0,0,0]\n",
        "        # print(k)\n",
        "        hot[k] = 1\n",
        "        li[:,i] = hot\n",
        "    return li\n",
        "Y_train = onehot(Y_train)\n",
        "Y_test = onehot(Y_test)"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prQNZ3dV8ksP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y_test = Y_test.transpose()\n",
        "Y_train = Y_train.transpose()"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6POKDbUgFl0P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3379c698-dae0-4416-ea11-80ab8b97c336"
      },
      "source": [
        "Y_test.shape"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1464, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqjzrcQD8rfx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "20d6055f-b8d7-4690-cd6f-7750f2b7ad34"
      },
      "source": [
        "X_train = np.array(X_train)\n",
        "X_train.shape"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(13176, 4974)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUAXnB_K6BbE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(1000, input_dim=X_train.shape[1], activation='relu'))\n",
        "model.add(Dropout(0.9))\n",
        "# model.add(Dense(300, activation='relu'))\n",
        "model.add(Dense(100, activation='relu'))\n",
        "model.add(Dropout(0.7))\n",
        "model.add(Dense(10,activation=\"relu\"))\n",
        "model.add(Dense(3,activation= \"sigmoid\"))\n",
        "checkpoint = ModelCheckpoint('model_tf_nn.h5', verbose=1, monitor='val_accuracy',save_best_only=True, mode='auto')"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouFDDIjV8QDD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CAhOMX038S5L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9877c314-affc-4edc-a1aa-aa145d0a0d00"
      },
      "source": [
        "history = model.fit(X_train, Y_train,validation_data = (X_test,Y_test),callbacks=[checkpoint],epochs=20, batch_size=64)"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "202/206 [============================>.] - ETA: 0s - loss: 0.6904 - accuracy: 0.6231\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.61612, saving model to model_tf_nn.h5\n",
            "206/206 [==============================] - 1s 5ms/step - loss: 0.6882 - accuracy: 0.6239 - val_loss: 0.5317 - val_accuracy: 0.6161\n",
            "Epoch 2/20\n",
            "197/206 [===========================>..] - ETA: 0s - loss: 0.5115 - accuracy: 0.6886\n",
            "Epoch 00002: val_accuracy improved from 0.61612 to 0.82377, saving model to model_tf_nn.h5\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.5105 - accuracy: 0.6925 - val_loss: 0.4687 - val_accuracy: 0.8238\n",
            "Epoch 3/20\n",
            "191/206 [==========================>...] - ETA: 0s - loss: 0.3924 - accuracy: 0.8361\n",
            "Epoch 00003: val_accuracy improved from 0.82377 to 0.82445, saving model to model_tf_nn.h5\n",
            "206/206 [==============================] - 1s 4ms/step - loss: 0.3909 - accuracy: 0.8371 - val_loss: 0.3906 - val_accuracy: 0.8245\n",
            "Epoch 4/20\n",
            "197/206 [===========================>..] - ETA: 0s - loss: 0.3457 - accuracy: 0.8607\n",
            "Epoch 00004: val_accuracy did not improve from 0.82445\n",
            "206/206 [==============================] - 1s 4ms/step - loss: 0.3474 - accuracy: 0.8596 - val_loss: 0.3866 - val_accuracy: 0.8238\n",
            "Epoch 5/20\n",
            "205/206 [============================>.] - ETA: 0s - loss: 0.3159 - accuracy: 0.8736\n",
            "Epoch 00005: val_accuracy improved from 0.82445 to 0.83060, saving model to model_tf_nn.h5\n",
            "206/206 [==============================] - 1s 5ms/step - loss: 0.3171 - accuracy: 0.8732 - val_loss: 0.3888 - val_accuracy: 0.8306\n",
            "Epoch 6/20\n",
            "195/206 [===========================>..] - ETA: 0s - loss: 0.2909 - accuracy: 0.8833\n",
            "Epoch 00006: val_accuracy improved from 0.83060 to 0.83743, saving model to model_tf_nn.h5\n",
            "206/206 [==============================] - 1s 4ms/step - loss: 0.2936 - accuracy: 0.8815 - val_loss: 0.3878 - val_accuracy: 0.8374\n",
            "Epoch 7/20\n",
            "194/206 [===========================>..] - ETA: 0s - loss: 0.2747 - accuracy: 0.8914\n",
            "Epoch 00007: val_accuracy did not improve from 0.83743\n",
            "206/206 [==============================] - 1s 4ms/step - loss: 0.2734 - accuracy: 0.8918 - val_loss: 0.3938 - val_accuracy: 0.8292\n",
            "Epoch 8/20\n",
            "201/206 [============================>.] - ETA: 0s - loss: 0.2579 - accuracy: 0.8982\n",
            "Epoch 00008: val_accuracy did not improve from 0.83743\n",
            "206/206 [==============================] - 1s 4ms/step - loss: 0.2583 - accuracy: 0.8977 - val_loss: 0.4039 - val_accuracy: 0.8299\n",
            "Epoch 9/20\n",
            "193/206 [===========================>..] - ETA: 0s - loss: 0.2391 - accuracy: 0.9072\n",
            "Epoch 00009: val_accuracy did not improve from 0.83743\n",
            "206/206 [==============================] - 1s 4ms/step - loss: 0.2418 - accuracy: 0.9063 - val_loss: 0.4106 - val_accuracy: 0.8327\n",
            "Epoch 10/20\n",
            "193/206 [===========================>..] - ETA: 0s - loss: 0.2210 - accuracy: 0.9148\n",
            "Epoch 00010: val_accuracy did not improve from 0.83743\n",
            "206/206 [==============================] - 1s 4ms/step - loss: 0.2211 - accuracy: 0.9149 - val_loss: 0.4240 - val_accuracy: 0.8286\n",
            "Epoch 11/20\n",
            "206/206 [==============================] - ETA: 0s - loss: 0.2105 - accuracy: 0.9192\n",
            "Epoch 00011: val_accuracy did not improve from 0.83743\n",
            "206/206 [==============================] - 1s 4ms/step - loss: 0.2105 - accuracy: 0.9192 - val_loss: 0.4287 - val_accuracy: 0.8313\n",
            "Epoch 12/20\n",
            "201/206 [============================>.] - ETA: 0s - loss: 0.2043 - accuracy: 0.9217\n",
            "Epoch 00012: val_accuracy did not improve from 0.83743\n",
            "206/206 [==============================] - 1s 4ms/step - loss: 0.2036 - accuracy: 0.9219 - val_loss: 0.4412 - val_accuracy: 0.8251\n",
            "Epoch 13/20\n",
            "192/206 [==========================>...] - ETA: 0s - loss: 0.1889 - accuracy: 0.9323\n",
            "Epoch 00013: val_accuracy did not improve from 0.83743\n",
            "206/206 [==============================] - 1s 4ms/step - loss: 0.1882 - accuracy: 0.9328 - val_loss: 0.4748 - val_accuracy: 0.8245\n",
            "Epoch 14/20\n",
            "193/206 [===========================>..] - ETA: 0s - loss: 0.1745 - accuracy: 0.9351\n",
            "Epoch 00014: val_accuracy did not improve from 0.83743\n",
            "206/206 [==============================] - 1s 4ms/step - loss: 0.1737 - accuracy: 0.9350 - val_loss: 0.4606 - val_accuracy: 0.8258\n",
            "Epoch 15/20\n",
            "192/206 [==========================>...] - ETA: 0s - loss: 0.1595 - accuracy: 0.9420\n",
            "Epoch 00015: val_accuracy did not improve from 0.83743\n",
            "206/206 [==============================] - 1s 4ms/step - loss: 0.1608 - accuracy: 0.9410 - val_loss: 0.4921 - val_accuracy: 0.8204\n",
            "Epoch 16/20\n",
            "202/206 [============================>.] - ETA: 0s - loss: 0.1519 - accuracy: 0.9433\n",
            "Epoch 00016: val_accuracy did not improve from 0.83743\n",
            "206/206 [==============================] - 1s 4ms/step - loss: 0.1521 - accuracy: 0.9435 - val_loss: 0.5127 - val_accuracy: 0.8204\n",
            "Epoch 17/20\n",
            "206/206 [==============================] - ETA: 0s - loss: 0.1419 - accuracy: 0.9469\n",
            "Epoch 00017: val_accuracy did not improve from 0.83743\n",
            "206/206 [==============================] - 1s 4ms/step - loss: 0.1419 - accuracy: 0.9469 - val_loss: 0.5236 - val_accuracy: 0.8204\n",
            "Epoch 18/20\n",
            "205/206 [============================>.] - ETA: 0s - loss: 0.1412 - accuracy: 0.9498\n",
            "Epoch 00018: val_accuracy did not improve from 0.83743\n",
            "206/206 [==============================] - 1s 4ms/step - loss: 0.1411 - accuracy: 0.9498 - val_loss: 0.5141 - val_accuracy: 0.8231\n",
            "Epoch 19/20\n",
            "191/206 [==========================>...] - ETA: 0s - loss: 0.1280 - accuracy: 0.9531\n",
            "Epoch 00019: val_accuracy did not improve from 0.83743\n",
            "206/206 [==============================] - 1s 4ms/step - loss: 0.1270 - accuracy: 0.9533 - val_loss: 0.5450 - val_accuracy: 0.8217\n",
            "Epoch 20/20\n",
            "203/206 [============================>.] - ETA: 0s - loss: 0.1248 - accuracy: 0.9552\n",
            "Epoch 00020: val_accuracy did not improve from 0.83743\n",
            "206/206 [==============================] - 1s 4ms/step - loss: 0.1246 - accuracy: 0.9552 - val_loss: 0.5543 - val_accuracy: 0.8190\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7r_OzRtZOvoe",
        "colab_type": "text"
      },
      "source": [
        "# Highest Accuracy Acheived 83.743"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eK9Lf3mlPv4R",
        "colab_type": "text"
      },
      "source": [
        "Lets Try PCA for the above vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKBURG0CObpH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "decomp = PCA(n_components=400)\n",
        "decomp.fit(X_tf_df)\n",
        "X_train = decomp.transform(X_train)\n",
        "X_test =  decomp.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5LScQpbP6EI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(200, input_dim=X_train.shape[1], activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "# model.add(Dense(300, activation='relu'))\n",
        "model.add(Dense(100, activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(10,activation=\"relu\"))\n",
        "model.add(Dense(3,activation= \"sigmoid\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "539emGKRRxKg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nztQ2AsR6TX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 689
        },
        "outputId": "3fca7057-9ae1-43e8-a7b7-be7eee387b74"
      },
      "source": [
        "history = model.fit(X_train, Y_train,validation_data = (X_test,Y_test), epochs=20, batch_size=64)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "206/206 [==============================] - 1s 3ms/step - loss: 0.6526 - accuracy: 0.6844 - val_loss: 0.4622 - val_accuracy: 0.7883\n",
            "Epoch 2/20\n",
            "206/206 [==============================] - 1s 3ms/step - loss: 0.4298 - accuracy: 0.8022 - val_loss: 0.4354 - val_accuracy: 0.8019\n",
            "Epoch 3/20\n",
            "206/206 [==============================] - 1s 3ms/step - loss: 0.4092 - accuracy: 0.8092 - val_loss: 0.4327 - val_accuracy: 0.8033\n",
            "Epoch 4/20\n",
            "206/206 [==============================] - 1s 3ms/step - loss: 0.3979 - accuracy: 0.8207 - val_loss: 0.4316 - val_accuracy: 0.7978\n",
            "Epoch 5/20\n",
            "206/206 [==============================] - 0s 2ms/step - loss: 0.3866 - accuracy: 0.8254 - val_loss: 0.4359 - val_accuracy: 0.7978\n",
            "Epoch 6/20\n",
            "206/206 [==============================] - 1s 3ms/step - loss: 0.3792 - accuracy: 0.8314 - val_loss: 0.4272 - val_accuracy: 0.7999\n",
            "Epoch 7/20\n",
            "206/206 [==============================] - 0s 2ms/step - loss: 0.3666 - accuracy: 0.8386 - val_loss: 0.4323 - val_accuracy: 0.8067\n",
            "Epoch 8/20\n",
            "206/206 [==============================] - 1s 3ms/step - loss: 0.3575 - accuracy: 0.8405 - val_loss: 0.4438 - val_accuracy: 0.7999\n",
            "Epoch 9/20\n",
            "206/206 [==============================] - 0s 2ms/step - loss: 0.3466 - accuracy: 0.8487 - val_loss: 0.4439 - val_accuracy: 0.8012\n",
            "Epoch 10/20\n",
            "206/206 [==============================] - 1s 3ms/step - loss: 0.3304 - accuracy: 0.8550 - val_loss: 0.4330 - val_accuracy: 0.8005\n",
            "Epoch 11/20\n",
            "206/206 [==============================] - 1s 2ms/step - loss: 0.3202 - accuracy: 0.8637 - val_loss: 0.4457 - val_accuracy: 0.8060\n",
            "Epoch 12/20\n",
            "206/206 [==============================] - 1s 3ms/step - loss: 0.3035 - accuracy: 0.8704 - val_loss: 0.4596 - val_accuracy: 0.8026\n",
            "Epoch 13/20\n",
            "206/206 [==============================] - 1s 2ms/step - loss: 0.2902 - accuracy: 0.8786 - val_loss: 0.4611 - val_accuracy: 0.8060\n",
            "Epoch 14/20\n",
            "206/206 [==============================] - 1s 3ms/step - loss: 0.2750 - accuracy: 0.8841 - val_loss: 0.4753 - val_accuracy: 0.8033\n",
            "Epoch 15/20\n",
            "206/206 [==============================] - 1s 3ms/step - loss: 0.2632 - accuracy: 0.8899 - val_loss: 0.4831 - val_accuracy: 0.7999\n",
            "Epoch 16/20\n",
            "206/206 [==============================] - 1s 3ms/step - loss: 0.2480 - accuracy: 0.8983 - val_loss: 0.4772 - val_accuracy: 0.8053\n",
            "Epoch 17/20\n",
            "206/206 [==============================] - 0s 2ms/step - loss: 0.2360 - accuracy: 0.9042 - val_loss: 0.5020 - val_accuracy: 0.8060\n",
            "Epoch 18/20\n",
            "206/206 [==============================] - 1s 3ms/step - loss: 0.2276 - accuracy: 0.9082 - val_loss: 0.4977 - val_accuracy: 0.7992\n",
            "Epoch 19/20\n",
            "206/206 [==============================] - 1s 3ms/step - loss: 0.2120 - accuracy: 0.9136 - val_loss: 0.5178 - val_accuracy: 0.8067\n",
            "Epoch 20/20\n",
            "206/206 [==============================] - 1s 3ms/step - loss: 0.2041 - accuracy: 0.9171 - val_loss: 0.5449 - val_accuracy: 0.7978\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGKVHcNhScVq",
        "colab_type": "text"
      },
      "source": [
        "This alos does not help much. Let's try our new (self trained)word2vec embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9wboFcF9gEj",
        "colab_type": "text"
      },
      "source": [
        "#### For lstm and word embeddings based approaches please refer to the next .ipynb file\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_BhWUg-fdkY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}